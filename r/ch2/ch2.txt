## let's try out some of the labs from the Wikle cressie book

## we'll run through their code mostly as is, though I may
## use pandas instead of R for the data-wrangling, then 
## follow up with R packages

## then we back up and try to repeat the graphics with d3JS. 

## the js stuff we'll try to use observables.

## that may be too excruciatingly slow, but let's see. 

## start a repo so we which computers...


git remote add origin https://github.com/danchurch/bayesSpatioTemp.git
git branch -M main
git remote set-url origin git@github.com:danchurch/bayesSpatioTemp.git

git push -u origin main

## the book has some custom packages:

## the "book" package:
library(devtools)
install_github("andrewzm/STRbook")

## there is also a deprecated package, spatiotemporal
## looks like they failed to keep things up to spec for 
## cran. 

## download the archived version, and install it 

df="/home/daniel/Downloads/SpatioTemporal_1.1.7.tar.gz"
sudo R CMD INSTALL --build $df 

## now can we do our homework?

## to get the data in R:

library('dplyr') 
library('tidyr') 
library('STRbook')

system.file("extdata", "Stationinfo.dat", package="STRbook")

locs <- read.table(system.file("extdata", "Stationinfo.dat", package="STRbook"), 
                    col.names = c("id", "lat", "lon"))

times <- read.table(system.file("extdata", "Times_1990.dat", package="STRbook"), 
                    col.names = c("julian", "year", "month", "day"))

tmax <- read.table(system.file("extdata", "Tmax_1990.dat", package="STRbook"))
                    
names(times)

colnames(times)

names(tmax) <- locs$id

tmax <- cbind(times, tmax)

tmax_long <- 

## um...they are losing me here. 
## oh god. This is a mess. R sucks for
## cleaning up data. Tomorrow, let's do this python.

###############

## okay...let's do data wrangling our way. with pandas. 

## the goal here is to get the standard R packages for
## space time data the format they need, from the 
## raw data given by the book. Working backward, we need the 
## following packages in R:

library(sp)
library('spacetime')

## we have two types of dataframe here, and STIDF, 
## a space-time "irregular" df, and a STFDF, =
## space time "full" df

## the STI, we use the NOAA data. 

## STIDF needs a sp object, a date tag for each row of the sp object, 
## for example, from the docs:

library(sp)

sp = cbind(x = c(0,0,1), y = c(0,1,1))
row.names(sp) = paste("point", 1:nrow(sp), sep="")
sp = SpatialPoints(sp)

time = as.POSIXct("2010-08-05")+3600*(10:13)
m = c(10,20,30) # means for each of the 3 point locations
mydata = rnorm(length(sp)*length(time),mean=rep(m, 4))
IDs = paste("ID",1:length(mydata))

mydata = data.frame(values = signif(mydata,3), ID=IDs)

stidf = as(STFDF(sp, time, mydata), "STIDF")

## does this also work?

stidf2 = STIDF(sp=sp, 
                time=time, 
                data=mydata)
## no, figure out why later...

stidf[1:2,]

all.equal(stidf, stidf[stidf,]) ## why this?

## any way, an stidf is a multi-dimensional object,
## with base dimensions of number of spatial points
## by number of time points, and a data matrix with as many rows 
## as this spatial sites x time matrix (so 3 sites, 4 time points
## equals a data matrix with 12 rows, and as many columns as
## we have data for). Makes sense. So for the example data 
## in the book, the real chore is figuring out all the data 
## they want to include:

## finding these in R goes like this:
## locs 
system.file("extdata", "Stationinfo.dat", package="STRbook") 
## times
system.file("extdata", "Times_1990.dat", package="STRbook")
## tmax
system.file("extdata", "Tmax_1990.dat", package="STRbook")
## tmin
## TDP (dewpoint temp)
## precip
## any they are all in:

"/usr/local/lib/R/site-library/STRbook/extdata/"

## switch over to python, play with them there

import os
import numpy as np
import pandas as pd
import datetime


## so we want to make a dataframe for our spatial data, assume two colums:

ddir = "/usr/local/lib/R/site-library/STRbook/extdata/"
stationInfo = (pd.read_csv(ddir+'Stationinfo.dat', header=None)
                .iloc[:,0]
                .str.split(expand=True)
                .astype('float')
                )
stationInfo.columns=['id','lat','long']
stationInfo['id'] = stationInfo['id'].astype('int')
## data
## so now temp data?
times = (pd.read_csv(ddir+'Times_1990.dat', header=None)
                .iloc[:,0]
                .str.split(expand=True)
    )
times.columns = ['julian','year', 'month', 'day']
## this could be useful later
pd.to_datetime(times[['year', 'month', 'day']])
## weather data is in a couple different tables:
## temp max
tmax = (pd.read_csv(ddir+'Tmax_1990.dat', header=None)
                .iloc[:,0]
                .str.split(expand=True)
                .astype('int32')
    )
tmax.columns = stationInfo['id']
## temp min
tmin = (pd.read_csv(ddir+'Tmin_1990.dat', header=None)
                .iloc[:,0]
                .str.split(expand=True)
                .astype('int32')
    )
tmin.columns = stationInfo['id']
## temp dewpoint
tdp = (pd.read_csv(ddir+'TDP_1990.dat', header=None)
                .iloc[:,0]
                .str.split(expand=True)
                .astype('float')
    )
tdp.columns = stationInfo['id']
## precip
precip = (pd.read_csv(ddir+'Precip_1990.dat', header=None)
                .iloc[:,0]
                .str.split(expand=True)
                .astype('float')
    )
precip.columns = stationInfo['id']

## save for later
stationInfo.to_csv('stationInfo.csv')
times.to_csv('times.csv')
tmax.to_csv('tmax.csv')
tdp.to_csv('tdp.csv')
tmin.to_csv('tmin.csv')
precip.to_csv('precip.csv')

tmax.iloc[0:3,0:3]

tdp.iloc[0:3,0:3]

tmax.iloc[0:3,0:4]

precip.iloc[0:3,0:4]

stationInfo

stationInfo.shape

stationInfo.iloc[0:3,0:4]

times.iloc[0:3,:]

times.shape

tmax.shape

stationInfo.shape

## now we want all of these in long format, with timestamp and station id

dfi = precip
mmv = -99.989998
proc = 'precip'
aa = pd.concat([times,dfi], axis=1)
## melt
bb = pd.melt(aa, id_vars=['julian','year','month','day'],
    var_name='id', value_name='z')
## get rid of missing observations:
mask = bb.z <= mmv
cc = bb[~mask]
## add a column indicating the kind of data this is
cc['proc'] = proc

def makeLong(dfi, mmv, proc):
    aa = pd.concat([times,dfi], axis=1)
    bb = pd.melt(aa, id_vars=['julian','year','month','day'],
        var_name='id', value_name='z')
    mask = bb.z <= mmv
    cc = bb[~mask]
    cc['proc'] = proc
    return(cc)

dd = makeLong(precip, -99.989998, 'precip')

dd.equals(cc)

tdp.head()

## seems to work. Try on our other dfs?
## these are the various weather data

tmax_l = makeLong(tmax, -998, 'tmax')
tdp_l = makeLong(tdp, -999.90001, 'tdp')
tmin_l = makeLong(tmin, -9999, 'tmin')
precip_l = makeLong(precip, -99.989998, 'precip')

## seems to work. Author says to put them 
## all together:
NOAA_df_1990 = pd.concat([tmax_l, tdp_l, tmin_l, precip_l])

NOAA_df_1990.to_csv('NOAA_df_1990.csv', index=False)

NOAA_df_1990 = pd.read_csv('NOAA_df_1990.csv')

## change the time stamp data format, 
## the spatiotemp packages want yyyy-mm-dd:

#### back to R to create sptemp data objects:

library("sp")
library("spacetime")

## focus on the tmax data. I think we need a 
## single date column, properly formatted

tmax_l = makeLong(tmax, -998, 'tmax')
tmax_l['date']= pd.to_datetime(tmax_l[['year', 'month', 'day']], yearfirst=True)
## clean it up a bit
tmax_l = tmax_l[['date', 'id', 'z']]
## add latlong
stationInfo.set_index('id', inplace=True)
tmax_l['lat'] = tmax_l['id'].apply(lambda x: stationInfo.loc[x][0])
tmax_l['long'] = tmax_l['id'].apply(lambda x: stationInfo.loc[x][1])

## is this basically what spacetemp needs?

tmax_l.to_csv('tmax_l.csv')

## in R ##

## the goal here is to create a spatioTemporalIrregularDataFrame (stidf)

library(sp)
library(spacetime)

## use sp to make sp object

tmax_l <- read.csv('ch2/tmax_l.csv')

head(tmax_l)


## I guess there are two ways to make the stidf: stConstruct and STIDF

head(tmax_l[,c("date","z","lat","long")])

tmax_l <- (tmax_l[,c("date","z","lat","long")])

tmax_l[,'date']

class(tmax_l$date)

tmax_l$date <- as.Date(tmax_l$date)

tmax_l$date

stObj <- stConstruct(x = tmax_l, 
    space = c("long","lat"),
    time = "date")

## that worked

## I guess we can also do this?

spat_part <- SpatialPoints(coords = tmax_l[, c("long", "lat")])
## longitude first? weird.
temp_part <- tmax_l[,'date']
tmaxZ <- tmax_l[,'z'] 

stObj2 <- STIDF(sp = spat_part,
        time = temp_part,
        data = tmax_l)

tmaxspdf <- stObj2

#save(tmaxspdf, file='tmaxspdf.rda')

### great. we can also do a full, "regular" stdf, but we have to
## use our missing data, so that no time slots are missing, etc.

## but I am not going to waste too much time on that for the 
## moment. A pain. 

## from here we are going to start the visualizations. Since I want to 
## use d3js, this is going to take forever. I will need to work through
## the R visualizations, then see how to create them with js. Slow as
## mud. 

## but maybe fun. 

## anyway, exercise 2

## visualize the max_temp data we just wrestled with

install.packages("animation")

library("animation")
library("dplyr")
library("ggplot2")
library("gstat")
library("maps")
library("STRbook")

set.seed(1)

data("NOAA_df_1990", package="STRbook")

Tmax <- filter(NOAA_df_1990, 
                proc == "Tmax" & 
                month %in% 5:9 &
                year == 1993)


Tmax %>% select(lon, lat, date, julian, z) %>% head()

Tmax$t <- Tmax$julian - 728049

## 

Tmax_1 <- subset(Tmax, t %in% c(1,15,30))

NOAA_plot <- ggplot(Tmax_1) +
    geom_point(aes(x = lon,y = lat,
                    colour = z),
                size = 2) +
    col_scale(name = "degF") +
    xlab("Longitude (deg)") +
    ylab("Latitude (deg)") +
    geom_path(data = map_data("state"),
        aes(x = long, y = lat, group = group)) +
    facet_grid(~date) +
    coord_fixed(xlim = c(-105, -75),
    ylim = c(25, 50)) +
    theme_bw()

NOAA_plot

## great. so as a first exercise, how would we plot this using
## d3js?

## this can be our first observables. 

https://observablehq.com/@danchurch

## bring in the books for tomorrow.  

## my brainn hurts looking at js again.

## what's the goal? Today let's run through a 
## simple scatterplot tutorial.
## maybe after that a map tutorial


## so the first step - try to plot a scatter plot of a 
## randomly generated normal errors around a 1:1 line

## let's make a csv here in python and put it observables:

import numpy as np
import pandas as pd

xx = pd.Series(np.linspace(1,100,100))
yy = pd.Series(np.random.normal(xx, 10))

zz = pd.concat([xx,yy], axis=1)
zz.columns = ['X','Y']
zz.to_csv('sampleData.csv', index=False)

## observables really is an additional layer of complicated 

## I have a lot of trouble keeping track of variable scopes,
## and keeping track of what is js, d3js, and observable language

## I really want to like it, but I can't right now. From here,
## it looks better to set up a local environment for development 
## and then publish on a website. 

## so let's try to remember how to do this

## I think chrome was the best for hard resets, do I have chrome?

wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb

sudo dpkg -i google-chrome-stable_current_amd64.deb
 
## we'll need to manually update chrome, unless we want to 
## add a google repo to our list. I don't, so...

sudo apt install google-chrome-stable

## great. we need an http server

## i think I remember that the npm http-server worked well,
## and also the busybox tools

## we should have busybox

busybox httpd

## it's in there. test it with some simple html:

busybox httpd -h ./proj -p 8080

## okay, what else? 
## start writing your scripts in the old miniserver environment. 

## find a website to test on. Either AA or 
## get IT people to give you a webpage that will allow 
## javascript

## so for now, work on visualizations using local tools...

## so a scatterplot in d3js. 

## we practice locally with a busybox server, and on a website using the 
## AA website

## so to start off, we have lab 2.2 in the sptemp book,
## which I think is some kind of chloropleth or something.
## so we're going to jump right into maps with d3js. 

## don't want to get lost in the R code they have.
## it's brittle and relies on unsupported packages,
## and ggplot, which I don't want to get into. 

## so let's copy and paste:

library("animation")
library("dplyr")
library("ggplot2")
library("gstat")
library("maps")
library("STRbook")


data("NOAA_df_1990", package = "STRbook")
# subset the data

Tmax <- filter(NOAA_df_1990,
proc == "Tmax" &
# only max temperature
month %in% 5:9 &
# May to September
year == 1993)
# year of 199
## new time variable 
Tmax$t <- Tmax$julian - 728049
# extract data for just the three days in may of 1993
Tmax_1 <- subset(Tmax, t %in% c(1, 15, 30))

NOAA_plot <- ggplot(Tmax_1) + # plot points
    geom_point(aes(x = lon,y = lat, # lon and lat
    colour = z), # attribute color
    size = 2) + # make all points larger
    col_scale(name = "degF") + # attach color scale
    xlab("Longitude (deg)") + # x-axis label
    ylab("Latitude (deg)") + # y-axis label
    geom_path(data = map_data("state"), # add US states map
        aes(x = long, y = lat, group = group)) + 
    facet_grid(~date) + # facet by time 
    coord_fixed (xlim = c(-105, -75),
    ylim = c(25, 50)) + # zoom in
    theme_bw()

print(NOAA_plot)

## whoah. This is going to be a challenge in d3js. 

## but that's kind of what I was looking for. 

## over to d3js, setup a new project folder for this

## first step local

## dev it here: cd /home/daniel/Documents/learn/spTemp/js/ch2/fig2.1

## we need to make our map panel limits fit those of the example map
## and put a background grid on it...

## and put data on it...(= first problem)

## put this on the repo:

write.csv(Tmax_1, file="Tmax_1.csv")
 
min(Tmax_1$lat) ## 32.13334
max(Tmax_1$lat) ## 45.86666

min(Tmax_1$lon) ## -99.96667

max(Tmax_1$lon) ## -80.03333

head(Tmax_1) 

dim(Tmax_1) 


dim(Tmax_1[Tmax_1$day == 1,])
dim(Tmax_1[Tmax_1$day == 15,])
dim(Tmax_1[Tmax_1$day == 30,])

## how can we constrain this plot to just the intersting area of the map?


## also we want three of these. Is this possible? Should we put them 
## all in one 

## I think for this we need start building the CSS 

## but maybe first, we need to clip the state polygons down to the interesting
## area. 

max(Tmax_1$lat)

## great. the d3js graph is close enough, move on.

######## fig 2.4 ###########

## new challenge - figure 2.4, a chloropleth of missouri income levels
## get the data, do we still have this somewhere?

data("BEA", package="STRbook")

head(MOcounties[c('long','lat','NAME10')])

aa <- MOcounties[MOcounties$NAME10 == "Clark, MO",]

plot(aa$long, aa$lat)

## copy and paste
g1 <- ggplot(MOcounties) +
    geom_polygon(aes(x = long, y = lat,     # county boundary
                     group = NAME10,        # county group
                     fill = log(X1970))) +  # log of income
    geom_path(aes(x = long, y = lat,        # county boundary
                  group = NAME10)) +        # county group
    fill_scale(limits = c(7.5,10.2),
               name = "log($)")  +
    coord_fixed() + ggtitle("1970") +       # annotations
    xlab("x (m)") + ylab("y (m)") + theme_bw()

county1 <- MOcounties[MOcounties$NAME10 == "Clark, MO"]

print(g1)

## doesn't work. fucking ggplot. Why does everyone 
## love that package so much?
## don't care. We have the image of the figure,
## we can build it without plotting in R

## where are these shapefiles? 
ftp://msdis.missouri.edu/pub/Administrative_Political_Boundaries/MO_2010_TIGER_Census_County_Boundaries_shp.zip

## the ftp link isn't working
## use their interactive site
https://www.census.gov/cgi-bin/geo/shapefiles/index.php?year=2021&layergroup=Counties+%28and+equivalent%29

## locally?
.libPaths()

## here, I think:
grep -r "STRbook" /usr/local/lib/R/site-library

?system.file

system.file(package="STRbook")

## great, and where do we get the income data?
data("BEA", package="STRbook")

## this I think is the geo data
data("MOcounties", package="STRbook")
## we'll get this elsewhere and reformat

##  

write.csv(BEA, file='/home/daniel/Desktop/MOincomes.csv')

## can we link these two spatially?

#### go to python ####

import matplotlib.pyplot as plt; plt.ion()
import pandas as pd
import geopandas as gpd
import re

incomes = pd.read_csv('/home/daniel/Documents/learn/spTemp/dataSets/ch2/MOincomes.csv').dropna()

counties = pd.read_csv('/home/daniel/Desktop/MOcounties.csv')

## these two should match:

incomes.NAME10.dtype

incomes.shape
counties.shape

incomes.NAME10

counties.NAME10

incomes.NAME10 = incomes.NAME10.astype('object')
counties.NAME10 = counties.NAME10.astype('object')

aa = counties.merge(incomes, on='NAME10', how='left')

aa.shape

## we need to prune this down to size, I think group by county, etc. 
## tomorrow. 

incomes.X1970 

counties.X1970 ## doesn't have

## I think we really are just using the counties file for it's it 
## geospatial info

## so we don't need this, we created a geojson with qgis from the 
## original census data

## so we just need to bind our income data to the d3 selection of the 
## that would mean hosting this incomes csv somewhere, and binding 
## on the key of the county names, I guess. 

## were the county names conserved well enough among all the different datasets the to do this?

## can geopandas read urls?:

url="https://raw.githubusercontent.com/danchurch/bayesSpatioTemp/main/dataSets/ch2/MissouriCounties.GeoJSON"
MOcounties = gpd.read_file(url)

incomes.shape

MOcounties.shape

## awesome. Do the values line up in the Name columns?

MOcounties.NAME.apply(lambda a: print(a))

MOcounties.NAME.apply(lambda a: incomes.NAME10.str.contains(a).any())

## so some reformatting is necessary.

## probably better to do this on the CSV side, it has the more 
## complex names and would be nice to simplify them for the JS

incomes.NAME10


incomes = pd.read_csv('/home/daniel/Documents/learn/spTemp/dataSets/ch2/MOincomes.csv').dropna()

pat = re.compile(', MO')
incomes.NAME10 = incomes.NAME10.str.replace(pat, '', regex=True)

## did that work?

aa = incomes.NAME10.apply(lambda x: MOcounties.NAME.str.contains(x).any()) 

incomes.NAME10[~aa] ## St. Louis is fucked.

MOcounties

MOcounties.NAME

## ugh, there is a St. Louis county and St. Louis city. 

## make sure these values line up, tomorrow. 

## it's tomorrow today now
## where were we...

## we need some clean data so we can bind our income data
## to our paths of the counties of missouri

## so get the two datasets:
url="https://raw.githubusercontent.com/danchurch/bayesSpatioTemp/main/dataSets/ch2/MissouriCounties.GeoJSON"
MOcounties = gpd.read_file(url)

incomes = pd.read_csv('/home/daniel/Documents/learn/spTemp/dataSets/ch2/MOincomes.csv').dropna()
pat = re.compile(', MO')
incomes.NAME10 = incomes.NAME10.str.replace(pat, '', regex=True)

## why don't we combine them here, it's simpler to do this 
## in python, I think:


aa = incomes.NAME10.apply(lambda x: MOcounties.NAME.str.contains(x).any()) ## st. louis city not matched. fine.

bb = MOcounties.NAME.apply(lambda x: incomes.NAME10.str.contains(x).any()) ## all the geographies have a match in the incomes
## so use this for the join


cc = MOcounties.merge(incomes, left_on='NAME', right_on='NAME10', how='left')

## did that work?

cc.isnull().any() ## some columns have nan. 

## let's see if they are the ones we care about?

interestingCols = ['NAME', 'geometry', 'X1970', 'X1980', 'X1990']

cc[interestingCols].isnull().any() ## nope. 

## so try this for our dataset for mapping:

countiesGeoInc = MOcounties.merge(incomes, left_on='NAME', right_on='NAME10', how='left')[interestingCols]

countiesGeoInc.head() ## looks okay. 

filt = countiesGeoInc['NAME'] == 'St. Louis'
countiesGeoInc[filt] ## as noted else where, there are two St. Louis-es

## any other repeats?



filt = countiesGeoInc['NAME'].duplicated()
countiesGeoInc[filt] ## as noted else where, there are two St. Louis-es

## are the rows identical?
countiesGeoInc.duplicated().any()

## nope, so the two St. Louises are slightly different. 


countiesGeoInc.plot()


plt.close('all')


countiesGeoInc.plot()
filt = countiesGeoInc['NAME'] == 'St. Louis'
countiesGeoInc[filt].plot(ax=plt.gca(), color='red') ## as noted else where, there are two St. Louis-es
gpd.GeoSeries(countiesGeoInc.iloc[111].geometry).plot(color='green', ax=plt.gca())

## looks like the smaller one is the "independent city" of St. Louis.
## but the income data doesn't differentiate between the city and the
## county, they have the same income values. 
## I think this could cause problems.
## so nix the city, for now

countiesGeoInc[filt]

help(countiesGeoInc.drop)

countiesGeoInc.drop(111, axis='index', inplace=True)

## so to repeat, the pipeline for creating this:

url="https://raw.githubusercontent.com/danchurch/bayesSpatioTemp/main/dataSets/ch2/MissouriCounties.GeoJSON"
MOcounties = gpd.read_file(url)
incomes = pd.read_csv('/home/daniel/Documents/learn/spTemp/dataSets/ch2/MOincomes.csv').dropna()
pat = re.compile(', MO')
incomes.NAME10 = incomes.NAME10.str.replace(pat, '', regex=True)
interestingCols = ['NAME', 'geometry', 'X1970', 'X1980', 'X1990']
countiesGeoInc = MOcounties.merge(incomes, left_on='NAME', right_on='NAME10', how='left')[interestingCols]
countiesGeoInc.drop(111, axis='index', inplace=True)

## export:
countiesGeoInc.to_file('/home/daniel/Documents/learn/spTemp/dataSets/ch2/countiesGeoInc.GeoJSON')

## looks okay. it still carved out the St. Louis area. 
## we can do with out it. 

## action is now in the js files

### fig2.11

## do some data wrangling to get the hovmiller plots going:

## we need a lat by daynumber, with a z value of max temp.
## do it python, R sucks


import matplotlib.pyplot as plt; plt.ion()
import pandas as pd
import geopandas as gpd
import re, os, json

os.chdir('/home/daniel/Documents/learn/spTemp/dataSets/ch2')
stationInfo = pd.read_csv('stationInfo.csv')[['id', 'lat', 'long']]
stationInfo[['id']] = stationInfo[['id']].astype('int')
stationInfo.set_index('id', inplace=True)
times = pd.read_csv('times.csv')[['julian','year','month','day']]
tmax = pd.read_csv('tmax.csv',index_col=0)
tdp = pd.read_csv('tdp.csv', index_col=0)
tmin = pd.read_csv('tmin.csv', index_col=0)
precip = (pd.read_csv('precip.csv', index_col=0))
precip.drop(precip.columns[[0]], axis=1, inplace=True )
tmin = (pd.read_csv('tmin.csv', index_col=0)
         .drop(tmin.columns[[0]], axis=1))

os.chdir('/home/daniel/Documents/learn/spTemp/r/ch2')

## what do we need here?

## start with longitude:

## they take the range of longitude values,
## make 25 even steps,

## and they do something similar for the tmax values
## for days 0-150 days on the y-axis  

stationInfo.head()

## so what do we need to do here? 

## assume that we need a df with each pixel as a row,
## with columns for day number, latitude, longitude, and max temp. 

## really though, we need two dfs. one with lat and one long

## how to do this...

## each row will then essentially be the station that is 
## closest station to a given lat or long. 

## so define these first:

##### ! wait! had to clean up stationInfo to just sites with 
##### data!! see below 

stationInfo.long

list(range( round(stationInfo.long.min()), round(stationInfo.long.max())+1))

xx = pd.Series(range( round(stationInfo.long.min()), round(stationInfo.long.max())+1))

## that is our x value
## we need to find the stations closest to each

## for one:
(stationInfo.long - xx[0]).min()
idxx = (stationInfo.long - xx[0]).idxmin()
stationInfo.loc[idxx].name

xx = (pd.Series(range( round(stationInfo.long.min()), 
          round(stationInfo.long.max())+1)))


(
(stationInfo.long - (xx[20])).abs()
                            .idxmin()
)

def getNearestStatLong(i):
    idxx = (stationInfo.long - i).abs().idxmin()
    return(idxx)

nearestStatLong = xx.apply( getNearestStatLong )

nearestStatLong 
## works. Later use this to get our desired climate data (max tmp?)

## repeat for latitude 
yy = (pd.Series(range( round(stationInfo.lat.min()), 
          round(stationInfo.lat.max())+1)))
def getNearestStatLat(i):
    idxy = (stationInfo.lat - i).abs().idxmin()
    return(idxy)

nearestStatLat = yy.apply( getNearestStatLat )
## ok

## but we need our max temp. They use the data from the 
## days between 01 may 1993 and 30 sept 1993

## I think that dataset started on 01.01.1990

## so the above dates probably amount to starting 2 5/12 years
## later. Then add 150 days. 
## so this is probably 

( 2 * 356 ) + (5/12)*(356)  ## ~860 days into the data set.

## so let's say rows 860 to 1010

summer1993tmax = tmax.iloc[860:1010]
## as noted below, there are many columns with no data (-9999)
## let's remove those here:
missDatCols = summer1993tmax.apply (lambda x: (x == -9999).all(), axis=0)
stationsMissingData = missDatCols.index[missDatCols]
stationsNoMissingData = missDatCols.index[~missDatCols]
summer1993tmax = summer1993tmax.loc[:,~missDatCols]
## we also need to clean up the stationInfo object:

stationInfo = stationInfo.loc[stationsNoMissingData.astype('int')]


## now regenerate nearestStatL*** variables. God this is confusing

## we can subset this using the stations associataed 
## with the xx, yy locations found above. But we need to
## redefine with the new stationInfo object that contains
## only those stations with tmax data for this period:

importantStats = nearestStatLong.append(nearestStatLat)


##### or a month later. So this project goes. 

## so, we have a bunch of missing data
## I don't really care as long as there is something to work with
## for a z/pixel value

## but there isn't for some of these


summer1993tmax[importantStats.astype('str')]


summer1993tmax['14927']

summer1993tmax['14927'].mode() ## -9999

(summer1993tmax['14927'] == -9999).all() ## all -999

for i,j in summer1993tmax.iteritems():
    print( j.apply( lambda x: (x == -9999)).all())

## subset to those we were hoping to use
justNearStations = summer1993tmax[importantStats.astype('str')]

## how many of these are missing data?
for i,j in justNearStations.iteritems():
    print(i, j.apply( lambda x: (x == -9999)).all())
## lots

## get some culprits:

missDats = pd.DataFrame(columns=['station','allMissing'])
for i,j in justNearStations.iteritems():
    testStation = pd.DataFrame({'station':[i],
                                'allMissing':[j.apply( lambda x: (x == -9999)).all()],
                               })
    missDats = pd.concat([missDats, testStation])

missDats.shape ## 36 rows
missDats.allMissing.sum() ## 21 stations are missing data. Jeezus

## how do we fill this in?

aa = pd.Series(dtype='int')


aa = pd.concat([aa, pd.Series([35])])

aa = pd.concat([aa, pd.Series([35], index=[5])])

## we need to remove all no-data columns and look again

summer1993tmax.shape
summer1993tmax.head()

summer1993tmax['94910'] ## data present
summer1993tmax['94911'] ## all data missing

missDatCols = summer1993tmax.apply (lambda x: (x == -9999).all(), axis=0)

missDatCols.sum() ## 195 stations have no data for this time period

stationsMissingData = missDatCols.index[missDatCols]
## so these should be the stations with data
summer1993tmaxNoMissing = summer1993tmax.loc[:,~missDatCols]

## is there still missing data?
(summer1993tmaxNoMissing == -9999).any().any() ## yup
(summer1993tmaxNoMissing == -9999).any().sum() ## 3 stations

## let's not worry about it. 

## so rerun our station finding pipeline above, after removal 
## of no data columns/stations

## the above code won't make sense anymore, because I 
## have removed the -9999 values above, from summer1993tmax

summer1993tmax

(summer1993tmax == -9999).any().sum()  ## looks good

## okay, rerun our nearest-station algorithm again

xx = (pd.Series(range( round(stationInfo.long.min()), 
          round(stationInfo.long.max())+1)))
def getNearestStatLong(i):
    idxx = (stationInfo.long - i).abs().idxmin()
    return(idxx)

nearestStatLong = xx.apply( getNearestStatLong )

yy = (pd.Series(range( round(stationInfo.lat.min()), 
          round(stationInfo.lat.max())+1)))
def getNearestStatLat(i):
    idxy = (stationInfo.lat - i).abs().idxmin()
    return(idxy)

nearestStatLat = yy.apply( getNearestStatLat )

## check these. 
yy[0]
nearestStatLat[0]
stationInfo.loc[nearestStatLat[0]]

yy.iloc[-1]
nearestStatLat.iloc[-1]
stationInfo.loc[nearestStatLat.iloc[-1]]

xx[0]
nearestStatLong[0]
stationInfo.loc[nearestStatLong[0]]

xx.iloc[-1]
nearestStatLong.iloc[-1]
stationInfo.long[nearestStatLong.iloc[-1]]

## looks sane.

## okay, so what do we need now?
## for each long or lat, we need a chronosequence of 
## 150 days

summer1993tmax.shape
summer1993tmax.head()
summer1993tmax.tail()

## should be as simple as selecting stations out of summer1993tmax
## then re-ordering by lat or long order

## let's work on latitude, as this is the more interesting 
## figure anyway

nearestStatLat.astype('str')

heatmapData = summer1993tmax[ nearestStatLat.astype('str') ]
heatmapData.reset_index(inplace=True, drop=True)

## this should be roughly what we need. but we need to 
## make the units more agreeable to our graphing needs.

## x-axis units simple - days from zero

help(heatmapData.reset_index)

## y-axis units need to be degrees. 
## should as simple as relabeling with our yy?


## but we need this in long form, melted:


heatmapData.head()
heatmapData.tail()

heatmapData.melt().head()
heatmapData.melt().tail()


help(heatmapData.melt)

heatMapMelted = heatmapData.melt().reset_index()
## but this loses the y values?

aa = (heatmapData
                    .reset_index()
                    #.melt()
                    )

heatMapMelted.head()

df.melt(id_vars=['A'], value_vars=['B', 'C'], ignore_index=False)


heatmapData.melt(id_vars=['A'], value_vars=['B', 'C']).head()

heatmapData.melt().tail()


heatMapMelted.columns = ['y','x','z']

heatMapMelted.head()

heatmapData.head()



## and this should be our matrix for js, right?
help(heatmapData.to_csv)

heatMapMelted.to_csv('/home/daniel/Documents/learn/spTemp/dataSets/ch2/latitude_hov_heatmapData.csv', index=False)

## not quite right. we need to retain the y values, how?

heatmapData

(
heatmapData
  .reset_index()
  .rename({'index':'y'}, axis='columns')
  .melt()
  .head()
)

help(heatmapData)

help(heatmapData.rename)

heatmapData.head()

heatMapMelted.head()

df = pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c'},
                   'B': {0: 1, 1: 3, 2: 5},
                   'C': {0: 2, 1: 4, 2: 6}})

df.melt(id_vars=['A'], value_vars=['B', 'C'], ignore_index=False)

df.melt(id_vars=['A'], value_vars=['B', 'C'])

######## lab 2.3 #########

## now to actually crunch some numbers (exploratory analysis)

## they use essentially the data set we just generated, I think...

## the first statistics are spatial "empirical" means.
## spatial empirical means are means where samples from 
## different times have been averaged out. 

## leaves a map, with each pixel/z-value being an average across
## times.

#### empirical spatial means ####

## we need to group our max temps from the dataset above 
## by unique lat-long combinations and get the vector of 
## maxTemp values at each day for each  

## we didn't retain the dates for each value in our 
## long-form matrix from above. 

## for spatial means, I guess it doesn't matter,
## unless for some reason you would want to unequally
## weight the dates.

## we will probably need to fix this for the temporal 
## means...to get something like figure 2.15

## let's try to build at least the day number (day 0-150)
## back into the data. We can play with dates some other 
## time, this is unnecessary complexity right now.

## also, we want to include all stations, whereas before 
## we just took the station nearest to each whole lat/long point

import matplotlib.pyplot as plt; plt.ion()
import pandas as pd
##import geopandas as gpd

## get data, clean up 
folder='/home/daniel/Documents/learn/spTemp/dataSets/ch2/'
stationInfo = pd.read_csv(folder+'stationInfo.csv')[['id', 'lat', 'long']]
stationInfo[['id']] = stationInfo[['id']].astype('int')
stationInfo.set_index('id', inplace=True)
tmax = pd.read_csv((folder+'tmax.csv'),index_col=0)
## subset to just the summer of 1993
summer1993tmax = tmax.iloc[860:1010]
## remove columns with no data (-9999)
## let's remove those here:
missDatCols = summer1993tmax.apply (lambda x: (x == -9999).all(), axis=0)
summer1993tmax = summer1993tmax.loc[:,~missDatCols]
stationInfo = stationInfo.loc[~(missDatCols.values),:]

## rather than grouping by location, we should be able to group by
## statin. Each station needs the average of its max temp column


aa = summer1993tmax.mean(axis='rows')
aa.index = aa.index.astype('int64')
aa.name = "meanSummerTmax"
geoMeanTmax = (pd.concat([stationInfo, aa], axis=1)
    .reset_index()
    .rename(columns={'index':'statID'})
     )


## now we want an array of objects.
## each object has station id, lat, and long

geoMeanTmax.head()

geoMeanTmax.to_csv("/home/daniel/Documents/learn/spTemp/dataSets/ch2/geoMeanTmax.csv",
                        index=False)

## something is weird, we have three data points under 
## 20 degrees. 

geoMeanTmax = pd.read_csv("/home/daniel/Documents/learn/spTemp/dataSets/ch2/geoMeanTmax.csv")

geoMeanTmax['meanSummerTmax'].min()

lowTempFilter = geoMeanTmax['meanSummerTmax'] < 20

lowTempFilter = geoMeanTmax['meanSummerTmax'] < 40

geoMeanTmax['meanSummerTmax'][lowTempFilter]

## let's get rid of these 

geoMeanTmax = geoMeanTmax[~lowTempFilter]

geoMeanTmax.to_csv("/home/daniel/Documents/learn/spTemp/dataSets/ch2/geoMeanTmax.csv",
                        index=False)

######## fig2.15 / temporal averages ###########

## now we need the same process, but with space averaged out.
## we also want the curves for each station, as
## an indicator of uncertainty, in the graphic

## we want the same set of dates, summer 1993
## so ...

python3

import pandas as pd
## get data, clean up 
## station locations
folder='/home/daniel/Documents/learn/spTemp/dataSets/ch2/'
stationInfo = pd.read_csv(folder+'stationInfo.csv')[['id', 'lat', 'long']]
stationInfo[['id']] = stationInfo[['id']].astype('int')
stationInfo.set_index('id', inplace=True)
## temperature data
tmax = pd.read_csv((folder+'tmax.csv'),index_col=0)
## subset to just the summer of 1993
summer1993tmax = tmax.iloc[860:1010]
## remove columns with no data (-9999)
## let's remove those here:
missDatCols = summer1993tmax.apply (lambda x: (x == -9999).all(), axis=0)
summer1993tmax = summer1993tmax.loc[:,~missDatCols]
stationInfo = stationInfo.loc[~(missDatCols.values),:]
## that leaves some single -9999 values, convert to na
summer1993tmax[summer1993tmax == -9999] = None
## we want a curve that travels the mean of 
## each date across all stations.
grandTmaxMean = summer1993tmax.mean(axis='columns')
grandTmaxMean.index = grandTmaxMean.index.astype('int64')
grandTmaxMean.index.name = 'day'
grandTmaxMean.name = 'temp'

grandTmaxMean

## for ease of export I guess we can combine these into one csv:
#summer1993tmax ['grandMean'] = grandTmaxMean
## but we will have to separate those again, and it will be a 
## pain in js. 

## can we save out grandTmaxMean as a json?
## meh. goes on the wrong axis, hurts my head

dataDir="/home/daniel/Documents/learn/spTemp/dataSets/ch2/"

grandTmaxMean.to_csv(dataDir+'grandTmaxMean.csv')

grandTmaxMean.head()


## looks okay.
## while we are here, try the larger dataset as a csv:

grandTmaxMean.head()

summer1993tmax.head()

## looks like we want to transpose, so we can 
## create an array of station objects:

aa = summer1993tmax.transpose()

aa.iloc[0:5,0:5]

aa.index

summer1993tmax.transpose().to_csv("/home/daniel/Documents/learn/spTemp/dataSets/ch2/summer1993tmax.csv",
                        index=False)

##summer1993tmax.to_csv("/home/daniel/Documents/learn/spTemp/dataSets/ch2/summer1993tmax.csv",
##                        index=False)

tempMeanTmax = (pd.concat([stationInfo, grandTmaxMean], axis=0)
    .reset_index()
    .rename(columns={'index':'statID'})
     )

tempMeanTmax.head()

tempMeanTmax.tail()

## okay, now we need all the other station curves:

